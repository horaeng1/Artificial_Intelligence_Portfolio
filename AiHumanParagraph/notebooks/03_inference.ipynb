{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention-MIL + LoRA Inference\n",
        "학습된 모델을 불러와 테스트 데이터에 대한 추론을 수행하고 submission 파일을 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab 환경 설정\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \"transformers>=4.40,<5\" \"peft>=0.11.0\" accelerate scikit-learn tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 라이브러리 import 및 설정\n",
        "import os, re, gc, random, math, numpy as np, pandas as pd, torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# 메모리 파편화 완화\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# 시드/디바이스\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device, \" | Torch:\", torch.__version__)\n",
        "\n",
        "# BF16 설정\n",
        "USE_BF16 = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def autocast_ctx():\n",
        "    if not torch.cuda.is_available():\n",
        "        return nullcontext()\n",
        "    try:\n",
        "        dtype = torch.bfloat16 if USE_BF16 else torch.float16\n",
        "        return torch.autocast(device_type=\"cuda\", dtype=dtype)\n",
        "    except TypeError:\n",
        "        return torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "BASE_MODEL   = \"monologg/koelectra-base-v3-discriminator\"\n",
        "MAX_LEN      = 192\n",
        "POS_WEIGHT   = 14.23  # 학습시 사용된 값 (참고용)\n",
        "\n",
        "# 경로 설정\n",
        "DIR = ''\n",
        "OUTPUT_DIR = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 정의\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.electra import ElectraModel\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "class MILPooler(nn.Module):\n",
        "    \"\"\"Pooling 선택지: 'attn' | 'lse' | 'max' | 'mean'\"\"\"\n",
        "    def __init__(self, d, mode=\"lse\", r=128, lse_tau=10.0):\n",
        "        super().__init__()\n",
        "        self.mode = mode; self.tau = lse_tau\n",
        "        if mode == \"attn\":\n",
        "            self.W = nn.Linear(d, r)\n",
        "            self.u = nn.Linear(r, 1, bias=False)\n",
        "    \n",
        "    def forward(self, H):  # H: [n_i, d]\n",
        "        if self.mode == \"mean\":\n",
        "            z = H.mean(dim=0)\n",
        "            alpha = torch.full((H.size(0),), 1.0/H.size(0), device=H.device)\n",
        "            return z, alpha\n",
        "        if self.mode == \"max\":\n",
        "            idx = torch.argmax(H.norm(dim=1))\n",
        "            z = H[idx]\n",
        "            alpha = torch.zeros(H.size(0), device=H.device); alpha[idx] = 1.0\n",
        "            return z, alpha\n",
        "        if self.mode == \"lse\":\n",
        "            e = H.norm(dim=1) * self.tau\n",
        "            alpha = torch.softmax(e, dim=0)\n",
        "            z = torch.sum(alpha.unsqueeze(-1) * H, dim=0)\n",
        "            return z, alpha\n",
        "        # attention\n",
        "        A = torch.tanh(self.W(H))\n",
        "        e = self.u(A).squeeze(-1)\n",
        "        alpha = torch.softmax(e, dim=0)\n",
        "        z = torch.sum(alpha.unsqueeze(-1) * H, dim=0)\n",
        "        return z, alpha\n",
        "\n",
        "\n",
        "class AttentionMILModel(nn.Module):\n",
        "    def __init__(self, encoder: nn.Module, attn_r=128, pos_weight=1.0,\n",
        "                 pool_mode=\"lse\", lse_tau=10.0, inst_aux_lambda=0.1, inst_aux_k=1):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        if hasattr(self.encoder.config, \"use_cache\"):\n",
        "            self.encoder.config.use_cache = False\n",
        "        d = self.encoder.config.hidden_size\n",
        "        self.instance_head = nn.Linear(d, 1)\n",
        "        self.pool = MILPooler(d, mode=pool_mode, r=attn_r, lse_tau=lse_tau)\n",
        "        self.bag_head = nn.Linear(d, 1)\n",
        "        self.register_buffer(\"pos_weight\", torch.tensor(float(pos_weight)))\n",
        "        self.inst_aux_lambda = inst_aux_lambda\n",
        "        self.inst_aux_k = inst_aux_k\n",
        "\n",
        "    def encode_instances(self, input_ids, attention_mask, need_instance_logits=True):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        H = out.last_hidden_state[:, 0, :]\n",
        "        s = self.instance_head(H).squeeze(-1) if need_instance_logits else None\n",
        "        return H, s\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, bag_bounds, labels=None,\n",
        "                return_instance=True, return_alphas=False, **_):\n",
        "        H_all, s_all = self.encode_instances(input_ids, attention_mask, need_instance_logits=return_instance)\n",
        "        bag_logits, alphas_all = [], ([] if return_alphas else None)\n",
        "\n",
        "        for bi, (start, end) in enumerate(bag_bounds.tolist()):\n",
        "            H = H_all[start:end]\n",
        "            z, alpha = self.pool(H)\n",
        "            S = self.bag_head(z).squeeze(-1)\n",
        "            bag_logits.append(S)\n",
        "            if return_alphas:\n",
        "                alphas_all.append(alpha)\n",
        "\n",
        "        bag_logits = torch.stack(bag_logits, dim=0)\n",
        "        out = {\"bag_logits\": bag_logits}\n",
        "        if return_instance: out[\"instance_logits\"] = s_all\n",
        "        if return_alphas:   out[\"alphas\"] = alphas_all\n",
        "        return out\n",
        "\n",
        "print(\"모델 클래스 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "print(f\"토크나이저 로드 완료: {BASE_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 데이터 로드 및 Dataset 정의\n",
        "test_df = pd.read_csv(DIR + \"test.csv\")\n",
        "assert set([\"ID\", \"title\", \"paragraph_index\", \"paragraph_text\"]).issubset(test_df.columns)\n",
        "test_df[\"paragraph_text\"] = test_df[\"paragraph_text\"].astype(str).fillna(\"\")\n",
        "test_df = test_df.sort_values([\"title\", \"paragraph_index\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"테스트 데이터 로드 완료: {len(test_df)} rows\")\n",
        "\n",
        "class TestParagraphDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.groups = []\n",
        "        for title, g in df.groupby(\"title\"):\n",
        "            self.groups.append({\n",
        "                \"title\": title, \n",
        "                \"paras\": g[\"paragraph_text\"].tolist(), \n",
        "                \"index\": g.index.tolist()\n",
        "            })\n",
        "    def __len__(self): \n",
        "        return len(self.groups)\n",
        "    def __getitem__(self, idx): \n",
        "        return self.groups[idx]\n",
        "\n",
        "def collate_test(batch, tokenizer, max_len=256):\n",
        "    all_texts, bag_bounds, titles, indices = [], [], [], []\n",
        "    cursor = 0\n",
        "    for d in batch:\n",
        "        titles.append(d[\"title\"])\n",
        "        indices.append(d[\"index\"])\n",
        "        paras = d[\"paras\"] if d[\"paras\"] else [\"\"]\n",
        "        start = cursor\n",
        "        all_texts.extend(paras)\n",
        "        cursor += len(paras)\n",
        "        end = cursor\n",
        "        bag_bounds.append((start, end))\n",
        "    enc = tokenizer(all_texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
        "    return {\n",
        "        \"input_ids\": enc[\"input_ids\"], \n",
        "        \"attention_mask\": enc[\"attention_mask\"],\n",
        "        \"bag_bounds\": torch.tensor(bag_bounds, dtype=torch.long),\n",
        "        \"titles\": titles, \n",
        "        \"indices\": indices\n",
        "    }\n",
        "\n",
        "te_set = TestParagraphDataset(test_df)\n",
        "te_loader = DataLoader(\n",
        "    te_set, batch_size=1, shuffle=False,\n",
        "    collate_fn=lambda b: collate_test(b, tokenizer, MAX_LEN)\n",
        ")\n",
        "\n",
        "print(f\"테스트 DataLoader 생성 완료: {len(te_set)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 로드\n",
        "# LoRA 설정 (학습시 사용한 설정과 동일해야 함)\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    task_type=\"FEATURE_EXTRACTION\"\n",
        ")\n",
        "\n",
        "# Electra encoder + LoRA\n",
        "best_encoder = ElectraModel.from_pretrained(BASE_MODEL)\n",
        "best_encoder = get_peft_model(best_encoder, lora_cfg)\n",
        "\n",
        "# MIL 모델 생성\n",
        "best_model = AttentionMILModel(\n",
        "    encoder=best_encoder, \n",
        "    attn_r=128, \n",
        "    pos_weight=POS_WEIGHT,\n",
        "    pool_mode=\"lse\", \n",
        "    lse_tau=10.0, \n",
        "    inst_aux_lambda=0.1, \n",
        "    inst_aux_k=1\n",
        ").to(device)\n",
        "\n",
        "# 학습된 가중치 로드\n",
        "MODEL_PATH = OUTPUT_DIR + \"attnmil_lora_best6.pt\"\n",
        "best_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "print(f\"모델 로드 완료: {MODEL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference 함수 정의\n",
        "@torch.no_grad()\n",
        "def infer(model, loader, T: float = 1.0):\n",
        "    \"\"\"\n",
        "    문단별 확률을 예측하는 inference 함수\n",
        "    \n",
        "    Args:\n",
        "        model: 학습된 AttentionMILModel\n",
        "        loader: 테스트 DataLoader\n",
        "        T: Temperature scaling 값 (calibration용)\n",
        "    \n",
        "    Returns:\n",
        "        paragraph_probs: 각 문단의 AI 생성 확률\n",
        "    \"\"\"\n",
        "    paragraph_probs = np.zeros(len(test_df), dtype=float)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.inference_mode():\n",
        "        for batch in tqdm(loader, desc=\"Inference\", ncols=100):\n",
        "            batch = {k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()}\n",
        "            \n",
        "            with autocast_ctx():\n",
        "                out = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    bag_bounds=batch[\"bag_bounds\"],\n",
        "                    labels=None,\n",
        "                    return_instance=True,\n",
        "                    return_alphas=False,\n",
        "                )\n",
        "\n",
        "                # 문단 로짓을 float32로 캐스팅 후 sigmoid\n",
        "                inst_logits = out[\"instance_logits\"].float()\n",
        "                p_all = torch.sigmoid(inst_logits).detach().cpu().numpy()\n",
        "\n",
        "                (start, end) = batch[\"bag_bounds\"][0].tolist()\n",
        "                idxs = batch[\"indices\"][0]\n",
        "                paragraph_probs[idxs] = p_all[start:end]\n",
        "\n",
        "            del out, batch, inst_logits, p_all\n",
        "            torch.cuda.empty_cache()\n",
        "            try:\n",
        "                torch.cuda.ipc_collect()\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return paragraph_probs\n",
        "\n",
        "print(\"Inference 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference 실행\n",
        "best_T = 0.0385  # 학습시 최적화된 Temperature 값 (calibration)\n",
        "paragraph_probs = infer(best_model, te_loader, T=best_T)\n",
        "\n",
        "print(f\"Inference 완료: {len(paragraph_probs)} paragraphs\")\n",
        "print(f\"예측 확률 범위: [{paragraph_probs.min():.4f}, {paragraph_probs.max():.4f}]\")\n",
        "print(f\"예측 확률 평균: {paragraph_probs.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submission 파일 생성 및 저장\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"], \n",
        "    \"generated\": paragraph_probs\n",
        "})\n",
        "\n",
        "# ID 숫자 기준 정렬\n",
        "submission[\"id_num\"] = submission[\"ID\"].str.extract(r\"(\\d+)\").astype(int)\n",
        "submission = submission.sort_values(\"id_num\").drop(columns=\"id_num\").reset_index(drop=True)\n",
        "\n",
        "# CSV 저장\n",
        "SUBMISSION_PATH = OUTPUT_DIR + \"submission_inference.csv\"\n",
        "submission.to_csv(SUBMISSION_PATH, index=False)\n",
        "\n",
        "print(f\"Submission 저장 완료: {SUBMISSION_PATH}\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "submission.head(10)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
